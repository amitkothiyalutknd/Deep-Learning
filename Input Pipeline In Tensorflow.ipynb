{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tw\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-13T20:30:36.168718Z","iopub.execute_input":"2023-01-13T20:30:36.169373Z","iopub.status.idle":"2023-01-13T20:30:37.590473Z","shell.execute_reply.started":"2023-01-13T20:30:36.169280Z","shell.execute_reply":"2023-01-13T20:30:37.589617Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/256x256-images/train.zip\n/kaggle/input/256x256-images/masks.zip\n/kaggle/input/256x256-images/__results__.html\n/kaggle/input/256x256-images/__notebook__.ipynb\n/kaggle/input/256x256-images/__output__.json\n/kaggle/input/256x256-images/custom.css\n/kaggle/input/256x256-images/__results___files/__results___6_0.png\n/kaggle/input/image-recognition-gender-detection-inceptionv3/__results__.html\n/kaggle/input/image-recognition-gender-detection-inceptionv3/__output__.json\n/kaggle/input/image-recognition-gender-detection-inceptionv3/weights.best.inc.male.hdf5\n/kaggle/input/image-recognition-gender-detection-inceptionv3/custom.css\n/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___40_0.png\n/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___11_1.png\n/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___23_0.png\n/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___41_0.png\n/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___13_0.png\n","output_type":"stream"}]},{"cell_type":"code","source":"monthlySaleNumber = [25, 63, -185, 35, 67, 42,92, -35, 63, 56]","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.597738Z","iopub.execute_input":"2023-01-13T20:30:37.598645Z","iopub.status.idle":"2023-01-13T20:30:37.604873Z","shell.execute_reply.started":"2023-01-13T20:30:37.598585Z","shell.execute_reply":"2023-01-13T20:30:37.603173Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"twDatsts = tw.data.Dataset.from_tensor_slices(monthlySaleNumber)\ntwDatsts","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.606588Z","iopub.execute_input":"2023-01-13T20:30:37.607217Z","iopub.status.idle":"2023-01-13T20:30:37.629011Z","shell.execute_reply.started":"2023-01-13T20:30:37.607184Z","shell.execute_reply":"2023-01-13T20:30:37.628185Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2023-01-13 20:30:37.614756: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<TensorSliceDataset shapes: (), types: tf.int32>"},"metadata":{}}]},{"cell_type":"code","source":"for sales in twDatsts:\n    print(sales)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.631294Z","iopub.execute_input":"2023-01-13T20:30:37.631634Z","iopub.status.idle":"2023-01-13T20:30:37.642018Z","shell.execute_reply.started":"2023-01-13T20:30:37.631605Z","shell.execute_reply":"2023-01-13T20:30:37.641204Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"tf.Tensor(25, shape=(), dtype=int32)\ntf.Tensor(63, shape=(), dtype=int32)\ntf.Tensor(-185, shape=(), dtype=int32)\ntf.Tensor(35, shape=(), dtype=int32)\ntf.Tensor(67, shape=(), dtype=int32)\ntf.Tensor(42, shape=(), dtype=int32)\ntf.Tensor(92, shape=(), dtype=int32)\ntf.Tensor(-35, shape=(), dtype=int32)\ntf.Tensor(63, shape=(), dtype=int32)\ntf.Tensor(56, shape=(), dtype=int32)\n","output_type":"stream"}]},{"cell_type":"code","source":"for sales in twDatsts:\n    print(sales.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.643162Z","iopub.execute_input":"2023-01-13T20:30:37.643715Z","iopub.status.idle":"2023-01-13T20:30:37.653097Z","shell.execute_reply.started":"2023-01-13T20:30:37.643686Z","shell.execute_reply":"2023-01-13T20:30:37.651651Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"25\n63\n-185\n35\n67\n42\n92\n-35\n63\n56\n","output_type":"stream"}]},{"cell_type":"code","source":"for sales in twDatsts.as_numpy_iterator():\n    print(sales)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.654671Z","iopub.execute_input":"2023-01-13T20:30:37.655029Z","iopub.status.idle":"2023-01-13T20:30:37.665425Z","shell.execute_reply.started":"2023-01-13T20:30:37.655003Z","shell.execute_reply":"2023-01-13T20:30:37.664060Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"25\n63\n-185\n35\n67\n42\n92\n-35\n63\n56\n","output_type":"stream"}]},{"cell_type":"code","source":"for sales in twDatsts.take(5):\n    print(sales.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.667109Z","iopub.execute_input":"2023-01-13T20:30:37.667552Z","iopub.status.idle":"2023-01-13T20:30:37.679121Z","shell.execute_reply.started":"2023-01-13T20:30:37.667513Z","shell.execute_reply":"2023-01-13T20:30:37.677865Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"25\n63\n-185\n35\n67\n","output_type":"stream"}]},{"cell_type":"code","source":"twDatsts = twDatsts.filter(lambda x: x>0)\nfor sales in twDatsts.as_numpy_iterator():\n    print(sales)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.680268Z","iopub.execute_input":"2023-01-13T20:30:37.681012Z","iopub.status.idle":"2023-01-13T20:30:37.736347Z","shell.execute_reply.started":"2023-01-13T20:30:37.680978Z","shell.execute_reply":"2023-01-13T20:30:37.734964Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"25\n63\n35\n67\n42\n92\n63\n56\n","output_type":"stream"},{"name":"stderr","text":"2023-01-13 20:30:37.720724: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"twDatsts = twDatsts.map(lambda x: x*86)\nfor sales in twDatsts.as_numpy_iterator():\n    print(sales)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.737742Z","iopub.execute_input":"2023-01-13T20:30:37.738109Z","iopub.status.idle":"2023-01-13T20:30:37.776868Z","shell.execute_reply.started":"2023-01-13T20:30:37.738081Z","shell.execute_reply":"2023-01-13T20:30:37.775651Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"2150\n5418\n3010\n5762\n3612\n7912\n5418\n4816\n","output_type":"stream"}]},{"cell_type":"code","source":"twDatsts = twDatsts.shuffle(5)\nfor sales in twDatsts.as_numpy_iterator():\n    print(sales)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.777953Z","iopub.execute_input":"2023-01-13T20:30:37.778415Z","iopub.status.idle":"2023-01-13T20:30:37.803062Z","shell.execute_reply.started":"2023-01-13T20:30:37.778388Z","shell.execute_reply":"2023-01-13T20:30:37.802253Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"2150\n5418\n3010\n3612\n4816\n5418\n5762\n7912\n","output_type":"stream"}]},{"cell_type":"code","source":"for sales_batch in twDatsts.batch(4):\n    print(sales_batch.numpy()) ","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.804102Z","iopub.execute_input":"2023-01-13T20:30:37.805406Z","iopub.status.idle":"2023-01-13T20:30:37.827914Z","shell.execute_reply.started":"2023-01-13T20:30:37.805375Z","shell.execute_reply":"2023-01-13T20:30:37.827027Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[5418 2150 5762 7912]\n[5418 3010 3612 4816]\n","output_type":"stream"}]},{"cell_type":"code","source":"twDatsts = tw.data.Dataset.from_tensor_slices(monthlySaleNumber)\ntwDatsts = twDatsts.filter(lambda x: x>0).map(lambda y: y*86).shuffle(2).batch(2)\nfor sales in twDatsts.as_numpy_iterator():\n    print(sales)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.829410Z","iopub.execute_input":"2023-01-13T20:30:37.829752Z","iopub.status.idle":"2023-01-13T20:30:37.892737Z","shell.execute_reply.started":"2023-01-13T20:30:37.829721Z","shell.execute_reply":"2023-01-13T20:30:37.891370Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[5418 3010]\n[2150 3612]\n[5762 5418]\n[7912 4816]\n","output_type":"stream"}]},{"cell_type":"code","source":"imagesdst = tw.data.Dataset.list_files('/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/*', shuffle = False)\nfor file in imagesdst.take(5):\n    print(file.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.897379Z","iopub.execute_input":"2023-01-13T20:30:37.897677Z","iopub.status.idle":"2023-01-13T20:30:37.912441Z","shell.execute_reply.started":"2023-01-13T20:30:37.897651Z","shell.execute_reply":"2023-01-13T20:30:37.911035Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"b'/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___11_1.png'\nb'/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___13_0.png'\nb'/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___23_0.png'\nb'/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___40_0.png'\nb'/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___41_0.png'\n","output_type":"stream"}]},{"cell_type":"code","source":"imagesdst = imagesdst.shuffle(200)\nfor file in imagesdst.take(10):\n    print(file.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.913679Z","iopub.execute_input":"2023-01-13T20:30:37.914045Z","iopub.status.idle":"2023-01-13T20:30:37.926520Z","shell.execute_reply.started":"2023-01-13T20:30:37.914012Z","shell.execute_reply":"2023-01-13T20:30:37.925330Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"b'/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___40_0.png'\nb'/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___23_0.png'\nb'/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___11_1.png'\nb'/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___41_0.png'\nb'/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___13_0.png'\n","output_type":"stream"}]},{"cell_type":"code","source":"className = ['Livingthing', 'NonLivingthing']","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.927764Z","iopub.execute_input":"2023-01-13T20:30:37.928071Z","iopub.status.idle":"2023-01-13T20:30:37.934327Z","shell.execute_reply.started":"2023-01-13T20:30:37.928043Z","shell.execute_reply":"2023-01-13T20:30:37.933098Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"imgCount = len(imagesdst)\nimgCount","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.935647Z","iopub.execute_input":"2023-01-13T20:30:37.936071Z","iopub.status.idle":"2023-01-13T20:30:37.948351Z","shell.execute_reply.started":"2023-01-13T20:30:37.936035Z","shell.execute_reply":"2023-01-13T20:30:37.947294Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}]},{"cell_type":"code","source":"trainSize = int(imgCount * 0.8)\ntrainDS = imagesdst.take(trainSize)\ntestDS = imagesdst.skip(trainSize)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.949366Z","iopub.execute_input":"2023-01-13T20:30:37.949690Z","iopub.status.idle":"2023-01-13T20:30:37.957426Z","shell.execute_reply.started":"2023-01-13T20:30:37.949660Z","shell.execute_reply":"2023-01-13T20:30:37.956483Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"len(trainDS), len(testDS)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.958470Z","iopub.execute_input":"2023-01-13T20:30:37.959419Z","iopub.status.idle":"2023-01-13T20:30:37.969050Z","shell.execute_reply.started":"2023-01-13T20:30:37.959390Z","shell.execute_reply":"2023-01-13T20:30:37.968335Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(4, 1)"},"metadata":{}}]},{"cell_type":"code","source":"p = '/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___11_1.png'\np.split(\"/\")[-2]","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.970014Z","iopub.execute_input":"2023-01-13T20:30:37.970669Z","iopub.status.idle":"2023-01-13T20:30:37.979004Z","shell.execute_reply.started":"2023-01-13T20:30:37.970642Z","shell.execute_reply":"2023-01-13T20:30:37.978039Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'__results___files'"},"metadata":{}}]},{"cell_type":"code","source":"def get_label(file_path):\n    import os\n    return tw.strings.split(file_path, os.path.sep)[-2]\n\ndef process_image(file_path):\n    label = get_label(file_path)\n    img = tw.io.read_file(file_path)\n    img = tw.image.decode_jpeg(img)\n    img = tw.image.resize(img, [128, 128])\n    return img, label","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.980259Z","iopub.execute_input":"2023-01-13T20:30:37.981171Z","iopub.status.idle":"2023-01-13T20:30:37.989672Z","shell.execute_reply.started":"2023-01-13T20:30:37.981138Z","shell.execute_reply":"2023-01-13T20:30:37.988347Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"for t in trainDS.take(4):\n    print(t.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:37.991215Z","iopub.execute_input":"2023-01-13T20:30:37.991561Z","iopub.status.idle":"2023-01-13T20:30:38.009264Z","shell.execute_reply.started":"2023-01-13T20:30:37.991533Z","shell.execute_reply":"2023-01-13T20:30:38.008075Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"b'/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___13_0.png'\nb'/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___23_0.png'\nb'/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___41_0.png'\nb'/kaggle/input/image-recognition-gender-detection-inceptionv3/__results___files/__results___11_1.png'\n","output_type":"stream"}]},{"cell_type":"code","source":"trainDS = trainDS.map(process_image)\nfor img, label in trainDS.take(5):\n    print(\"Image:\", img)\n    print(\"Label:\", label)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:38.010951Z","iopub.execute_input":"2023-01-13T20:30:38.012703Z","iopub.status.idle":"2023-01-13T20:30:38.214518Z","shell.execute_reply.started":"2023-01-13T20:30:38.012655Z","shell.execute_reply":"2023-01-13T20:30:38.213547Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Image: tf.Tensor(\n[[[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n ...\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]], shape=(128, 128, 4), dtype=float32)\nLabel: tf.Tensor(b'__results___files', shape=(), dtype=string)\nImage: tf.Tensor(\n[[[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n ...\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]], shape=(128, 128, 4), dtype=float32)\nLabel: tf.Tensor(b'__results___files', shape=(), dtype=string)\nImage: tf.Tensor(\n[[[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n ...\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]], shape=(128, 128, 4), dtype=float32)\nLabel: tf.Tensor(b'__results___files', shape=(), dtype=string)\nImage: tf.Tensor(\n[[[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n ...\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]\n\n [[255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  ...\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]\n  [255. 255. 255.   0.]]], shape=(128, 128, 4), dtype=float32)\nLabel: tf.Tensor(b'__results___files', shape=(), dtype=string)\n","output_type":"stream"},{"name":"stderr","text":"Cleanup called...\nCleanup called...\nCleanup called...\nCleanup called...\n","output_type":"stream"}]},{"cell_type":"code","source":"def scale (image, label):\n    return image/255, label\n\ntrainDS = trainDS.map(scale)\nfor image, label in trainDS.take(5):\n    print(\"Image:\", img.numpy()[0][0])\n    print(\"Label:\", label.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-01-13T20:30:38.215997Z","iopub.execute_input":"2023-01-13T20:30:38.216942Z","iopub.status.idle":"2023-01-13T20:30:38.321900Z","shell.execute_reply.started":"2023-01-13T20:30:38.216904Z","shell.execute_reply":"2023-01-13T20:30:38.321136Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Image: [255. 255. 255.   0.]\nLabel: b'__results___files'\nImage: [255. 255. 255.   0.]\nLabel: b'__results___files'\nImage: [255. 255. 255.   0.]\nLabel: b'__results___files'\nImage: [255. 255. 255.   0.]\nLabel: b'__results___files'\n","output_type":"stream"},{"name":"stderr","text":"Cleanup called...\nCleanup called...\nCleanup called...\nCleanup called...\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}